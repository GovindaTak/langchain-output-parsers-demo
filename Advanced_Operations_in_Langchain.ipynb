{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced Operations in LangChain\n",
        "1) Cost or usage tracking 2)Caching implementations 3)Streaming of Output"
      ],
      "metadata": {
        "id": "0J6kMgL0eHTP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zZBBjteeDuq",
        "outputId": "d7900193-e097-4569-f09e-1cdbef41d8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.61 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.62-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.81.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain_openai) (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.61->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain_openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n",
            "Downloading langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.62-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.4/438.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.60\n",
            "    Uninstalling langchain-core-0.3.60:\n",
            "      Successfully uninstalled langchain-core-0.3.60\n",
            "Successfully installed langchain-core-0.3.62 langchain_openai-0.3.18\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.62)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "OPENAI_API_KEY=getpass(\"Enter the Api key :-\")\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ef3J3X7fFXI",
        "outputId": "a5512374-5bad-4131-887d-9fe2d2c08ef4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Api key :-··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "llm=OpenAI(model='gpt-4o-mini',temperature=0)"
      ],
      "metadata": {
        "id": "PXzZc4hUflSq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__1) Tracking the cost or usage__"
      ],
      "metadata": {
        "id": "m--j6vpJf4yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "input=\"explain the nemo guardrail framework.\"\n",
        "prompt=PromptTemplate.from_template(\"{input}\")"
      ],
      "metadata": {
        "id": "hKSyN5ssf2y7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=prompt | llm"
      ],
      "metadata": {
        "id": "Ly485wB9gYlh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.callbacks import get_openai_callback\n",
        "with get_openai_callback() as cb:\n",
        "  response=chain.invoke(input=input)\n",
        "  print(response)\n",
        "  print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPTEIqSWgjfA",
        "outputId": "34ecd69d-118d-4c16-d18e-51e5cbf190d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are its components and how does it work?**\n",
            "\n",
            "The Nemo Guardrail framework is a safety and governance tool designed to ensure that AI systems operate within defined ethical and operational boundaries. It is particularly relevant in the context of large language models (LLMs) and other AI systems that can generate content or make decisions based on input data.\n",
            "\n",
            "### Components of the Nemo Guardrail Framework:\n",
            "\n",
            "1. **Guardrails**: These are predefined rules and constraints that guide the behavior of the AI system. They can include ethical guidelines, operational limits, and safety protocols that the AI must adhere to while functioning.\n",
            "\n",
            "2. **Monitoring**: Continuous monitoring of the AI's outputs and actions is essential to ensure compliance with the established guardrails. This can involve real-time analysis of the AI's decisions and the content it generates.\n",
            "\n",
            "3. **Feedback Mechanism**: The framework includes a feedback loop that allows for the collection of data on the AI's performance. This feedback can be used to refine the guardrails, improve the AI's behavior, and address any issues that arise.\n",
            "\n",
            "4. **User Interface**: A user-friendly interface is crucial for stakeholders to interact with the AI system, understand its outputs, and provide input on its performance. This interface can also facilitate the adjustment of guardrails\n",
            "Tokens Used: 264\n",
            "\tPrompt Tokens: 8\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 256\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0001548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb.total_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch5ztWu7hQ09",
        "outputId": "5721d5d7-ca23-49ec-e537-253e95df1e8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001548"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cb.total_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dedocxhgiATB",
        "outputId": "24ee0745-0123-406e-8ae0-7b61ef645ea2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__2) Caching implementation__\n",
        "various types of chache can be implement for any type of llm ,we are going through the 2 types of cache :- InMemory cache and SQLite Cache.\n",
        "\n",
        "A)InMemory Cache"
      ],
      "metadata": {
        "id": "1sRlWCX5iRJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "set_llm_cache(InMemoryCache())"
      ],
      "metadata": {
        "id": "T83m5W_kiECm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain.invoke(input=input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "tiH-0h8rj10C",
        "outputId": "24af1279-8450-4dd7-c751-a093ef947d96"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.3 ms, sys: 5.05 ms, total: 31.4 ms\n",
            "Wall time: 5.32 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" What are its components and how does it work?**\\n\\nThe Nemo Guardrail framework is a set of guidelines and tools designed to ensure the safe and responsible use of AI systems, particularly in the context of large language models (LLMs). The framework aims to mitigate risks associated with AI deployment, such as harmful outputs, misinformation, and ethical concerns.\\n\\n**Components of the Nemo Guardrail Framework:**\\n\\n1. **Safety Measures:** These are protocols and mechanisms put in place to prevent the AI from generating harmful or inappropriate content. This includes content filtering, toxicity detection, and adherence to community guidelines.\\n\\n2. **Ethical Guidelines:** The framework emphasizes the importance of ethical considerations in AI development and deployment. This includes fairness, accountability, transparency, and respect for user privacy.\\n\\n3. **User Feedback Mechanisms:** Nemo encourages the incorporation of user feedback to continuously improve the AI system. This can involve user reporting features, surveys, and other methods to gather insights on the AI's performance and impact.\\n\\n4. **Monitoring and Evaluation:** Regular monitoring and evaluation of the AI's outputs and behavior are crucial. This involves analyzing the AI's performance, identifying potential issues, and making necessary adjustments to the system.\\n\\n5. **Training and Fine-tuning:** The framework advocates for ongoing training and fine\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain.invoke(input=input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "QT0brTEykDc2",
        "outputId": "ff7df2d1-4564-4881-cd18-708f36713c1e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 864 µs, sys: 0 ns, total: 864 µs\n",
            "Wall time: 853 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" What are its components and how does it work?**\\n\\nThe Nemo Guardrail framework is a set of guidelines and tools designed to ensure the safe and responsible use of AI systems, particularly in the context of large language models (LLMs). The framework aims to mitigate risks associated with AI deployment, such as harmful outputs, misinformation, and ethical concerns.\\n\\n**Components of the Nemo Guardrail Framework:**\\n\\n1. **Safety Measures:** These are protocols and mechanisms put in place to prevent the AI from generating harmful or inappropriate content. This includes content filtering, toxicity detection, and adherence to community guidelines.\\n\\n2. **Ethical Guidelines:** The framework emphasizes the importance of ethical considerations in AI development and deployment. This includes fairness, accountability, transparency, and respect for user privacy.\\n\\n3. **User Feedback Mechanisms:** Nemo encourages the incorporation of user feedback to continuously improve the AI system. This can involve user reporting features, surveys, and other methods to gather insights on the AI's performance and impact.\\n\\n4. **Monitoring and Evaluation:** Regular monitoring and evaluation of the AI's outputs and behavior are crucial. This involves analyzing the AI's performance, identifying potential issues, and making necessary adjustments to the system.\\n\\n5. **Training and Fine-tuning:** The framework advocates for ongoing training and fine\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Conclusion :-__ we can see how much time difference when same prompt given as input."
      ],
      "metadata": {
        "id": "cZ00afUEkxmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) SQLITE Cache"
      ],
      "metadata": {
        "id": "S-Wxy3r9mk9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache(SQLiteCache(database_path=\"langchain.db\"))"
      ],
      "metadata": {
        "id": "1zFZ3Wbckeq7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"Explain about the SQLiteCache.\"\n"
      ],
      "metadata": {
        "id": "zDf03NJnnhQS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llm.invoke(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "mO3hAgSGnuag",
        "outputId": "41da0331-711f-4796-ae50-b16f5dfeab0d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.5 ms, sys: 2.22 ms, total: 26.8 ms\n",
            "Wall time: 4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' What is it? How does it work? What are its advantages and disadvantages?**\\n\\nSQLiteCache is a caching mechanism that uses SQLite as the underlying storage engine. It is designed to store frequently accessed data in a local SQLite database, allowing for faster retrieval of data compared to fetching it from a remote source or recalculating it.\\n\\n**How it works:**\\n1. **Data Storage:** When data is requested, the cache first checks if the data is available in the SQLite database.\\n2. **Cache Hit:** If the data is found (cache hit), it is retrieved from the SQLite database and returned to the requester.\\n3. **Cache Miss:** If the data is not found (cache miss), the application fetches the data from the original source (e.g., an API or a remote database), stores it in the SQLite database, and then returns it to the requester.\\n4. **Expiration and Invalidation:** The cache may implement expiration policies to remove stale data or provide mechanisms to invalidate specific entries when the underlying data changes.\\n\\n**Advantages:**\\n- **Speed:** Accessing data from a local SQLite database is generally faster than fetching it from a remote source.\\n- **Persistence:** Data stored in SQLite is persistent, meaning it can survive application restarts.\\n- **'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response=chain.invoke(input=input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6GjMrNwn0Pl",
        "outputId": "275363e3-0d93-46ed-a45c-1d3745a4e634"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 850 µs, sys: 0 ns, total: 850 µs\n",
            "Wall time: 839 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njzybM3aoQ2M",
        "outputId": "74038cf6-6f5c-4c20-cb91-f020380e0abd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is it? How does it work? What are its advantages and disadvantages?**\n",
            "\n",
            "SQLiteCache is a caching mechanism that uses SQLite as the underlying storage engine. It is designed to store frequently accessed data in a local SQLite database, allowing for faster retrieval of data compared to fetching it from a remote source or recalculating it.\n",
            "\n",
            "**How it works:**\n",
            "1. **Data Storage:** When data is requested, the cache first checks if the data is available in the SQLite database.\n",
            "2. **Cache Hit:** If the data is found (cache hit), it is retrieved from the SQLite database and returned to the requester.\n",
            "3. **Cache Miss:** If the data is not found (cache miss), the application fetches the data from the original source (e.g., an API or a remote database), stores it in the SQLite database, and then returns it to the requester.\n",
            "4. **Expiration and Invalidation:** The cache may implement expiration policies to remove stale data or provide mechanisms to invalidate specific entries when the underlying data changes.\n",
            "\n",
            "**Advantages:**\n",
            "- **Speed:** Accessing data from a local SQLite database is generally faster than fetching it from a remote source.\n",
            "- **Persistence:** Data stored in SQLite is persistent, meaning it can survive application restarts.\n",
            "- **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(\"langchain.db\")"
      ],
      "metadata": {
        "id": "Lqxi6fK4odKv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__3) Streaming LLM Output__"
      ],
      "metadata": {
        "id": "_mcF1fRjqgEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chain.stream(input=\"Explain streaming in langchain.\"):\n",
        "  print(chunk,end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AS2BlfaqIFr",
        "outputId": "99460355-04a3-498e-b0e8-76a836ddbdfe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are the benefits of using streaming in langchain? Provide an example of how to implement streaming in a LangChain application.\n",
            "\n",
            "Streaming in LangChain refers to the ability to receive and process data in real-time as it is generated, rather than waiting for the entire dataset to be available before starting to process it. This is particularly useful in applications that require immediate feedback or interaction, such as chatbots, real-time data analysis, or any scenario where low latency is critical.\n",
            "\n",
            "### Benefits of Using Streaming in LangChain\n",
            "\n",
            "1. **Real-time Interaction**: Streaming allows for immediate responses, making applications feel more interactive and responsive to user inputs.\n",
            "  \n",
            "2. **Reduced Latency**: By processing data as it arrives, applications can reduce the time it takes to deliver results, which is crucial for user experience.\n",
            "\n",
            "3. **Resource Efficiency**: Streaming can be more efficient in terms of memory and processing power, as it allows for processing smaller chunks of data rather than loading everything into memory at once.\n",
            "\n",
            "4. **Scalability**: Streaming can help applications scale better, as they can handle incoming data continuously without needing to wait for batch processing.\n",
            "\n",
            "5. **Improved User Experience**: Users can see results incrementally, which can enhance engagement and satisfaction.\n",
            "\n",
            "### Example of"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sro6Jq9urGIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}